from typing import List, Dict, Any, Tuple
from xarray import Dataset
import base64
import xml.etree.ElementTree as ElTree
import struct
from datetime import datetime

# from kExceptions import KameleontImportError
# from kXarrayMethods import make_dimension_array, make_data_array, make_dataset
# from import_modules.kImporterSuper import ImporterSuper
# from kDbCommands import generate_knc_filename
# from kInfoJsonCommands import LibraryLayer, LibraryLayerProperty, create_update_json
import numpy as np
import pandas as pd


periodic_table_list = ['H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl',
                       'Ar', 'K', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As',
                       'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd',
                       'In', 'Sn', 'Sb', 'Te', 'I', 'Xe', 'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm', 'Sm', 'Eu',
                       'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt',
                       'Au', 'Hg', 'Tl', 'Pb', 'Bi', 'Po', 'At', 'Rn', 'Fr', 'Ra', 'Ac', 'Th', 'Pa', 'U', 'Np',
                       'Pu', 'Am', 'Cm', 'Bk', 'Cf', 'Es', 'Fm', 'Md', 'No', 'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs',
                       'Mt', 'Ds', 'Rg', 'Cn', 'Nh', 'Fl', 'Mc', 'Lv', 'Ts', 'Og']
periodic_table_dict = {
    element_index + 1: element_name for element_index, element_name in enumerate(periodic_table_list)
}


def get_spectrum_hardware_params(data_root: ElTree.Element) -> Dict:
    """
    Method that walks the XML tree from the given root and finds the information for the hardware parameters
    that were stored in the spx file. Returns the found values as a dict.

    :param data_root: xml tree element that represents the root of the spx file to start walking the xml tree
    :type data_root: xml.etree.ElementTree.Element
    :return: dictionary containing the information of the hardware parameters from the spx file
    :rtype: Dict
    """
    spectrum_hardware_params_dict = {}

    search_str = "./ClassInstance/[@Type='TRTSpectrum']" \
                 "/TRTHeaderedClass" \
                 "/ClassInstance/[@Type='TRTSpectrumHardwareHeader']"
    spectrum_node = data_root.find(search_str)

    spectrum_hardware_params_dict['RealTime'] = float(spectrum_node.find('RealTime').text)
    spectrum_hardware_params_dict['LifeTime'] = float(spectrum_node.find('LifeTime').text)
    spectrum_hardware_params_dict['DeadTime'] = float(spectrum_node.find('DeadTime').text)
    spectrum_hardware_params_dict['ZeroPeakPosition'] = int(spectrum_node.find('ZeroPeakPosition').text)
    spectrum_hardware_params_dict['ZeroPeakFrequency'] = int(spectrum_node.find('ZeroPeakFrequency').text)
    spectrum_hardware_params_dict['PulseDensity'] = int(spectrum_node.find('PulseDensity').text)
    spectrum_hardware_params_dict['Amplification'] = int(float(spectrum_node.find('Amplification').text))
    spectrum_hardware_params_dict['ShapingTime'] = int(spectrum_node.find('ShapingTime').text)
    spectrum_hardware_params_dict['DetectorCount'] = int(spectrum_node.find('DetectorCount').text)
    spectrum_hardware_params_dict['SelectedDetectors'] = spectrum_node.find('SelectedDetectors').text
    return spectrum_hardware_params_dict


def get_position(data_root: ElTree.Element) -> np.array:
    """
    For some reason, the positions of the stages are only somtimes given explicitly within the spx file. If not,
    they can be extracted from another entry in the file. This entry looks like a string, but is encoded. The
    string was generated by taking the positions as floats and making a list of the bytes, that represent this.
    The bytes of the x, y and z-positions are appended in this list without delimiter, resulting in 3*8 bytes.
    These bytes are then base64 encoded and emerged in a larger string, in which the remaining data is unknown to
    me. The extraction of the position from this is the reverse of the above procedure. The section of the string
    that contains the positions was found out experimentally to start at character 121. The 24 characters are
    base64-decoded and the resulting characters are unpacked into a list of floats, which is converted into a
    np-array for further use.

    :param data_root: xml tree element that represents the root of the spx file to start walking the xml tree
    :type data_root: xml.etree.ElementTree.Element
    :return: numpy array of positions in order x, y, z
    :rtype: np.array
    """

    position_list = []
    search_str = "./ClassInstance/[@Type='TRTSpectrum']" \
                 "/TRTHeaderedClass" \
                 "/ClassInstance/[@Type='TRTAxesHeader']" \
                 "/AxesParameter/"
    if len(data_root.findall(search_str)) > 0:  # if positions are explicitly supplied in spx file
        for elem in data_root.findall(search_str):
            position_list.append(float(elem.attrib['AxisPosition']))
        return np.array(position_list)
    else:  # not supplied explicitly. Change searchstring and decode it
        search_str = "./ClassInstance/[@Type='TRTSpectrum']" \
                     "/TRTHeaderedClass" \
                     "/ClassInstance/[@Type='TRTUnknownHeader']/[@Name='RTREM']" \
                     "/Data"
        position_elem = data_root.find(search_str)
        # decode data into a string. Positions within the string are found experimentally
        position_string = base64.b64decode(position_elem.text.encode('ASCII'))[121:121 + 24]
        positions = np.array(struct.unpack('<ddd', position_string)).astype("float")
        return positions


def create_grid(in_array: np.array) -> Tuple[List, bool]:
    """
    Method that generate a grid with x-positions and y-positions from a 2D-array of all positions, at which
    measurements were taken. (Input contains a list of all "pairs" of absolute x- and y-positions; number of
    entries equals number of measurements. Output contains a list of all x-positions and all y-positions which
    span the grid of measurement points. No duplicates. x*y = number of measurements.

    :param in_array: 2D array of all measurement positions. in_array[x = 0, y = 1][No. spec]
    :type in_array: np.array
    :return: tuple of a List of arrays for x and y-positions and a bool stating, if the grid was measured in
    x-direction first (true) or in y-direction first (false) (rows vs. coulumns)
    :rtype:bool
    """
    epsilon = 0.01  # distance in mm that is considered to be approx. 0

    def treat_first_direction(p_diff_array: np.array, step_rough: float) -> np.array:
        """
        Method to extract the measurement positions for the direction, in which was scanned first (i.e. x or y).
        The calculation is based on the differences between the absolute position in subsequent measurement files.
        Since the scan was done in this direction first, most differences will be the step-size in this direction
        (within the precision of the setup). Only at the end of each line, a large step will be done for "carriage
        return" to the beginning of the line. The measurement positions of this can be built from the average of
        the most often occurring difference and the number of points, until a "carriage return" occurs.

        :param p_diff_array: 1D-array with differences between the absolute positions of subsequent measurements
        in this direction (x or y). If not a single line/column, duplicates (within the precision of the positioning
        of the device) will occur.
        :type p_diff_array: np.array
        :param step_rough: rough step size in this direction
        :type step_rough: float

        :return: 1D array with the positions of the scan in this directions (no duplicates even for multi-line
        :rtype: np.array
        """
        step_precision = 0.03
        # get step size precisely
        small_step_idxs = ((p_diff_array > (1 - step_precision) * step_rough)
                           & (p_diff_array < (1 + step_precision) * step_rough))
        step_posis = np.where(small_step_idxs)[0]
        step_aver = np.mean(p_diff_array[step_posis])

        if len(p_diff_array) > len(step_posis):
            # get number of points per line
            restart_line_step = np.median(np.delete(p_diff_array, step_posis))  # second most often diffs
            restart_line_idxs = ((p_diff_array > (1 - step_precision) * restart_line_step)
                                 & (p_diff_array < (1 + step_precision) * restart_line_step))
            restart_line_posis_ori = np.where(restart_line_idxs)[0]  # posi of the line restart in original array

            if np.std(np.diff(p_diff_array[restart_line_posis_ori])) < epsilon:
                spots_per_line = int(np.mean(np.diff(restart_line_posis_ori)))
            else:
                raise
        else:
            spots_per_line = len(p_diff_array) + 1
        return np.linspace(0, step_aver * (spots_per_line - 1), spots_per_line)

    def treat_second_direction(p_diff_array: np.array):
        """
        Method to extract the measurement positions for the direction, in which was scanned second (i.e. x or y).
        The calculation is based on the differences between the absolute position in subsequent measurement files.
        Since the scan was done in this direction second, most differences will be 0. These can be discarded. Only
        at the end of each line, one step will be done ("new line", or column), which equals the step size in this
        direction . The measurement positions of this can be built from the average of the non-zero difference and
        the count of these.

        :param p_diff_array: 1D-array with differences between the absolute positions of subsequent measurements
        in this direction (x or y). If not a single line/column, duplicates (within the precision of the positioning
        of the device) will occur.
        :type p_diff_array: np.array

        :return: 1D array with the positions of the scan in this directions (no duplicates even for multi-line
        :rtype: np.array
        """
        # remove 0 size "steps"
        non_move_posis = np.where((p_diff_array > -1 * epsilon) & (p_diff_array < epsilon))[0]
        p_diff_array = np.delete(p_diff_array, non_move_posis)
        if len(p_diff_array) == 0:  # there is no move being done. Measurement is one line/column
            return np.zeros(1)
        else:
            # get step size rougly first, then precise
            step_rough = np.median(p_diff_array)
            step_posis = np.where((p_diff_array > 0.9 * step_rough) & (p_diff_array < 1.1 * step_rough))[0]
            step_aver = np.mean(p_diff_array[step_posis])
            no_lines = len(step_posis) + 1
            return np.linspace(0, step_aver * (no_lines - 1), no_lines)

    diff_array = np.abs(np.diff(in_array))  # array with the differences in between
    step_rough_x = float(np.median(diff_array[0]))
    step_rough_y = float(np.median(diff_array[1]))
    if step_rough_x < epsilon:  # most step-sizes are zero
        x_first = False  # y was scanned first and x stayed constant
        y_array = treat_first_direction(diff_array[1], step_rough_y)
        x_array = treat_second_direction(diff_array[0])
    else:
        x_first = True
        x_array = treat_first_direction(diff_array[0], step_rough_x)
        y_array = treat_second_direction(diff_array[1])

    x_array = np.round(x_array, decimals=2)
    y_array = np.round(y_array, decimals=2)

    return [x_array, y_array], x_first


def get_system_settings(data_root: ElTree.Element) -> Dict:
    """
    Method that walks the XML tree from the given root and finds the information for the system settings
    that were stored in the spx file. Returns the found values as a dict.

    :param data_root: xml tree element that represents the root of the spx file to start walking the xml tree
    :type data_root: xml.etree.ElementTree.Element
    :return: dictionary containing the information of the system settings from the spx file
    :rtype: Dict
    """

    system_settings_dict = {}

    search_str = "./ClassInstance/[@Type='TRTSpectrum']" \
                 "/TRTHeaderedClass" \
                 "/ClassInstance/[@Type='TRTXrfHeader']"
    xrf_node = data_root.find(search_str)

    system_settings_dict['TubeType'] = xrf_node.find('TubeType').text
    system_settings_dict['TubeNumber'] = xrf_node.find('TubeNumber').text
    system_settings_dict['TubeProdDate'] = xrf_node.find('TubeProdDate').text
    system_settings_dict['Voltage'] = int(xrf_node.find('Voltage').text)
    system_settings_dict['Current'] = int(xrf_node.find('Current').text)
    system_settings_dict['Anode'] = int(xrf_node.find('Anode').text)
    system_settings_dict['TubeIncidentAngle'] = float(xrf_node.find('TubeIncidentAngle').text)
    system_settings_dict['TubeTakeOffAngle'] = float(xrf_node.find('TubeTakeOffAngle').text)
    system_settings_dict['TubeWindow,AtomicNumber'] = \
        int(xrf_node.find('TubeWindow').find('AtomicNumber').text)
    system_settings_dict['TubeWindow,Thickness'] = \
        int(float(xrf_node.find('TubeWindow').find('Thickness').text))
    system_settings_dict['Optic'] = xrf_node.find('Optic').text
    system_settings_dict['SpotSize'] = int(float(xrf_node.find('SpotSize').text))
    system_settings_dict['ExcitationAngle'] = float(xrf_node.find('ExcitationAngle').text)
    system_settings_dict['DetectionAngle'] = float(xrf_node.find('DetectionAngle').text)
    system_settings_dict['ExcitationPathLength'] = float(xrf_node.find('ExcitationPathLength').text)
    system_settings_dict['DetectionPathLength'] = float(xrf_node.find('DetectionPathLength').text)
    system_settings_dict['SolidAngleDetection'] = float(xrf_node.find('SolidAngleDetection').text)
    system_settings_dict['AzimutAngleAbs'] = float(xrf_node.find('AzimutAngleAbs').text)
    system_settings_dict['DetAzimutAngle'] = float(xrf_node.find('DetAzimutAngle').text)
    system_settings_dict['ChamberPressure'] = float(xrf_node.find('ChamberPressure').text)
    system_settings_dict['TiltAngle'] = float(xrf_node.find('TiltAngle').text)
    system_settings_dict['DetSpotSize'] = float(xrf_node.find('DetSpotSize').text)
    system_settings_dict['Atmosphere'] = xrf_node.find('Atmosphere').text
    return system_settings_dict


def get_spectrum_header(data_root: ElTree.Element) -> Dict:
    """
    Method that walks the XML tree from the given root and finds the information for the spectrum header
    that were stored in the spx file. Returns the found values as a dict.

    :param data_root: xml tree element that represents the root of the spx file to start walking the xml tree
    :type data_root: xml.etree.ElementTree.Element
    :return: dictionary containing the information of the spectrum header from the spx file
    :rtype: Dict
    """

    spectrum_header_dict = {}
    search_str = "./ClassInstance/[@Type='TRTSpectrum']" \
                 "/ClassInstance/[@Type='TRTSpectrumHeader']"
    spectrum_header_node = data_root.find(search_str)
    date_time_string = spectrum_header_node.find('Date').text + ' ' + spectrum_header_node.find('Time').text
    spectrum_header_dict['DateTime'] = pd.to_datetime(date_time_string).strftime('%Y-%m-%dT%H:%M:%S.%f')
    spectrum_header_dict['ChannelCount'] = int(spectrum_header_node.find('ChannelCount').text)
    spectrum_header_dict['CalibAbs'] = float(spectrum_header_node.find('CalibAbs').text)
    spectrum_header_dict['CalibLin'] = float(spectrum_header_node.find('CalibLin').text)
    spectrum_header_dict['SigmaAbs'] = float(spectrum_header_node.find('SigmaAbs').text)
    spectrum_header_dict['SigmaLin'] = float(spectrum_header_node.find('SigmaLin').text)
    return spectrum_header_dict


def get_channels(data_root: ElTree.Element) -> np.array:
    """
    Method that walks the XML tree from the given root and finds the channel entry, which contains the counts of
    the spectra. Apparently sometimes trailing zeros are not saved by the Bruker software, such that the number of
    counts in the list is not always const = 4096, thus it is padded with zeros here.

    :param data_root: xml tree element that represents the root of the spx file to start walking the xml tree
    :type data_root: xml.etree.ElementTree.Element
    :return: array that contains the counts
    :rtype: np.array
    """
    search_str = "./ClassInstance/[@Type='TRTSpectrum']" \
                 "/Channels"
    channel_node = data_root.find(search_str)
    spectrum = channel_node.text.split(',')  # list containing the counts
    return np.array(spectrum + ['0'] * (4096 - len(spectrum)), dtype=int)  # padding it with zeros for const. length


def is_results_in_file(data_root: ElTree.Element) -> bool:
    """
    Method that walks the XML tree from the given root and checks, if a node for fit-result exists.

    :param data_root: xml tree element that represents the root of the spx file to start walking the xml tree
    :type data_root: xml.etree.ElementTree.Element
    :return: true if results node exists, else false
    :rtype: bool
    """
    search_str = "./ClassInstance/[@Type='TRTSpectrum']" \
                 "/ClassInstance/[@Type='TRTResult']"
    if len(data_root.findall(search_str)) > 0:
        return True
    else:
        return False


def get_fit_bkg(data_root: ElTree.Element) -> np.array:
    """
    Method that walks the XML tree from the given root and finds the channel entry, which contains the counts of
    the fitted background. Since apparently sometimes trailing zeros are not saved by the Bruker software for the
    counts of the actual spectrum, such that the number of counts in the list is not always const = 4096,
    the zero padding, as performed for the actual counts of the spectrum, is applied to the background as well to be
    save.

    :param data_root: xml tree element that represents the root of the spx file to start walking the xml tree
    :type data_root: xml.etree.ElementTree.Element
    :return: array that contains the counts of the background
    :rtype: np.array
    """
    search_str = "./ClassInstance/[@Type='TRTSpectrum']" \
                 "/ChildClassInstances" \
                 "/ClassInstance/[@Type='TRTXRFMultiQuantificationResults']" \
                 "/TRTSpectrumQuantificationResults" \
                 "/ClassInstance/[@Type='TRTSpectrumList']" \
                 "/ChildClassInstances" \
                 "/ClassInstance/[@Type='TRTSpectrum'][@Name='Background']" \
                 "/Channels"
    channel_node = data_root.find(search_str)
    background = channel_node.text.split(',')  # list containing the counts
    # padding it with zeros for const. length
    return np.array(background + ['0'] * (4096 - len(background)), dtype=float)


def get_fit_layer_composition(data_root: ElTree.Element) -> Tuple[List[Dict], List[Dict]]:
    """
    Method that walks the XML tree from the given root and finds the

    :param data_root: xml tree element that represents the root of the spx file to start walking the xml tree
    :type data_root: xml.etree.ElementTree.Element
    :return: List of layer result dicts , List of layer property dicts
    :rtype: Tuple[List[Dict], List[Dict]]
    """
    search_str = "./ClassInstance/[@Type='TRTSpectrum']" \
                 "/ClassInstance/[@Type='TRTLayerResultList'][@Name='LayerResults']" \
                 "/ChildClassInstances" \
                 "/ClassInstance/[@Type='TRTLayerResult']"

    layer_result_node_list = data_root.findall(search_str)

    layer_results = []
    layer_props = []
    for layer_node in layer_result_node_list:
        layer_name = layer_node.get('Name')
        element_node_list = layer_node.findall('TRTResult/Result')
        for element_node in element_node_list:
            atomic_number = int(element_node.find('Atom').text)
            layer_result = {'LayerName': layer_name,
                            'Element': periodic_table_dict[atomic_number]}
            for element_results in element_node:
                layer_result[element_results.tag] = element_results.text
            layer_results.append(layer_result)
        layer_prop = {'LayerName': layer_name,
                      'Density': float(layer_node.find('Density').text),
                      'Thickness': float(layer_node.find('Thickness').text)}
        thickness_density_node = layer_node.find('ThicknessError')
        if thickness_density_node is not None:
            layer_prop['ThicknessError'] = float(thickness_density_node.text)
        else:
            layer_prop['ThicknessError'] = float('nan')
        layer_props.append(layer_prop)

    return layer_results, layer_props


def get_roi_results(data_root: ElTree.Element) -> List[Dict]:
    """
    Method that walks the XML tree from the given root and finds the

    :param data_root: xml tree element that represents the root of the spx file to start walking the xml tree
    :type data_root: xml.etree.ElementTree.Element
    :return: List with one dict per roi containing the result of this (elements) roi
    :rtype: List[Dict]
    """
    search_str = "./ClassInstance/[@Type='TRTSpectrum']" \
                 "/ClassInstance/[@Type='TRTResult'][@Name='Results']" \
                 "/RoiResults"

    roi_result_node_list = data_root.findall(search_str)

    roi_results = []
    for roi_result_node in roi_result_node_list:
        roi_result = {}
        for roi_detail in roi_result_node:
            roi_result[roi_detail.tag] = roi_detail.text
        roi_results.append(roi_result)

    return roi_results


def get_deconvolution_results(data_root: ElTree.Element) -> Tuple[str, List[Dict]]:
    """
    Method that walks the XML tree from the given root and finds the

    :param data_root: xml tree element that represents the root of the spx file to start walking the xml tree
    :type data_root: xml.etree.ElementTree.Element
    :return: deconvolution method as string, List with one dict per fluorescence line
    :rtype: Tuple[str, List[Dict]]
    """
    search_str = "./ClassInstance/[@Type='TRTSpectrum']" \
                 "ChildClassInstances" \
                 "/ClassInstance/[@Type='TRTXRFMultiQuantificationResults']" \
                 "/TRTSpectrumQuantificationResults" \
                 "/ClassInstance/[@Type='TRTDeconvolutionResultList']"

    deconvolution_result_main_node = data_root.find(search_str)
    deconvolution_method = deconvolution_result_main_node.find('DeconvMethod').text

    deconvolution_result_node_list = deconvolution_result_main_node.findall(
        "ChildClassInstances/ClassInstance/[@Type='TRTDeconvolutionResult']")

    deconvolution_results = []
    for deconvolution_result_node in deconvolution_result_node_list:
        element = periodic_table_dict[int(deconvolution_result_node.find('Element').text)]
        for line in deconvolution_result_node.findall('Line'):
            line_info = line.text.split(',')
            deconvolution_results.append({'Energy': line_info[1],
                                          'Counts': line_info[2],
                                          'Element': element,
                                          'Line': line_info[0]})

    return deconvolution_method, deconvolution_results


def read(file_paths: list):  # , required_params: Dict[str, Any]):  # , library_infos: List[Dict[str, Any]],
    # user: str):

    #
    # general idea:
    # Read data from all files into lists of dicts with an index (idx) for the file name.
    # can be multiple dicts per file, depending on the information.
    # These dicts are then transformed into pd.DataFrames the indexes of these are reset such that the
    # DataFrames are on the way to multi-dimensionality. Then they are transformed to xr.Datasets, now
    # containing multiple multidimensional data-cubes.
    # The axis for the finally returned (new) xr.Datasets are generated and the Datasets are constructed
    # by picking the afore mentioned data-cubes

    # read the information from spx files
    measurement_rows = []  # list of dicts containing measurement parameters
    positions = []  # list of np.arrays with the positions of each spectrum
    spectra = []  # list of np.arrays with the spectra

    deconvolution_rows = []  # list of dicts containing deconvolution results
    deconvolution_methods = []  # list of strings with the name of the deconvolution method (hopefully all the same)
    roi_rows = []  # list of dicts containing the roi results
    layer_rows = []  # list of dicts containing the layer results (atom_percent, weight_percent of element)
    layer_prop_rows = []  # list of dicts containing layer properties (density, thickness, ...)
    fit_bkg = []  # list of np.arrays with the fitted background
    for idx, spx_file in enumerate(file_paths):
        spx_tree = ElTree.parse(spx_file, parser=ElTree.XMLParser(encoding='WINDOWS-1252'))
        spx_root = spx_tree.getroot()
        # read measurement infos
        info_dict = {}  # dict containing the info, that will be used to append as a row in the measurement_data
        info_dict.update(get_spectrum_hardware_params(spx_root))
        info_dict.update(get_system_settings(spx_root))
        info_dict.update(get_spectrum_header(spx_root))
        info_dict['idx'] = idx
        measurement_rows.append(info_dict)
        # measurement_data = measurement_data.append(pd.Series(info_dict, name=idx))
        # read position and spectrum
        positions.append(get_position(spx_root))
        spectra.append(get_channels(spx_root))
        ##########################################################################################
        # if is_results_in_file(spx_root):
        #     deconvolution_method, deconvolution_result = get_deconvolution_results(spx_root)
        #     deconvolution_methods.append(deconvolution_method)
        #     for line_dict in deconvolution_result:
        #         line_dict['idx'] = idx
        #         deconvolution_rows.append(line_dict)

        #     roi_results = get_roi_results(spx_root)
        #     for element_dict in roi_results:
        #         element_dict['idx'] = idx
        #         roi_rows.append(element_dict)

        #     layer_results, layer_props = get_fit_layer_composition(spx_root)
        #     for layer_results_dict in layer_results:
        #         layer_results_dict['idx'] = idx
        #         layer_rows.append(layer_results_dict)

        #     for layer_prop_dict in layer_props:
        #         layer_prop_dict['idx'] = idx
        #         layer_prop_rows.append(layer_prop_dict)

        #     fit_bkg.append(get_fit_bkg(spx_root))
        ##########################################################################################
    # generate axes
    positions_array = np.array(positions).T  # make a (multi-dim) array from list of position and transpose
    position_axes, x_first = create_grid(positions_array)  # calculates grid positions in x and y direction

    # the grid will be with relative positions such that the point with lowest x and y will be at (0,0).
    # for positions relative to the substrate, the padding entered by the user will be added in the dims.
    # x_dim_array = make_dimension_array(dim_name="x",
    #                                    dim_values=position_axes[0] + required_params['start_pos_x'],
    #                                    dim_units="mm",
    #                                    dim_long_name="x position")
    # y_dim_array = make_dimension_array(dim_name="y",
    #                                    dim_values=position_axes[1] + required_params['start_pos_y'],
    #                                    dim_units="mm",
    #                                    dim_long_name="y position")

    measurement_data = pd.DataFrame(measurement_rows).set_index(['idx'])
    channel_numbers = np.arange(measurement_data['ChannelCount'].values[0])  # no. of channels is constant --> [any]
    # In the spx files, there are entries for a linear function of the channel to energy conversion
    energy_axis = measurement_data['CalibAbs'].mean() + channel_numbers * measurement_data['CalibLin'].mean()
    # energy_dim_array = make_dimension_array(
    #     dim_name="energy",
    #     dim_values=energy_axis,
    #     dim_units="eV",
    #     dim_long_name="fluorescence energy")

    # generate data arrays (energy first as it is most important, then in order of appearance in .spx)
    if x_first:  # x direction was scanned first, y stayed const until end of line
        len_x = len(position_axes[0])
        len_y = len(position_axes[1])
        order_letter = 'F'
    else:
        len_x = len(position_axes[1])
        len_y = len(position_axes[0])
        order_letter = 'C'
    return spectra, energy_axis, measurement_rows, positions_array, position_axes, (len_x, len_y, order_letter)

    # measurement_data_arrays_list = [
    #     make_data_array(data_name='intensity',
    #                     data=np.array(spectra).reshape((len_x, len_y, len(energy_axis)),
    #                                                    order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='counts',
    #                     data_long_name='fluorescence intensity',
    #                     additional_dims=[energy_dim_array]),
    #     make_data_array(data_name='realtime',
    #                     data=measurement_data['RealTime'].values.reshape((len_x, len_y),
    #                                                                      order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='ms',
    #                     data_long_name='XRF measurement real time',
    #                     additional_dims=None),
    #     make_data_array(data_name='lifetime',
    #                     data=measurement_data['LifeTime'].values.reshape((len_x, len_y),
    #                                                                      order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='ms',
    #                     data_long_name='XRF measurement life time',
    #                     additional_dims=None),
    #     make_data_array(data_name='deadtime',
    #                     data=measurement_data['DeadTime'].values.reshape((len_x, len_y),
    #                                                                      order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='ms',
    #                     data_long_name='XRF measurement dead time',
    #                     additional_dims=None),
    #     make_data_array(data_name='zero_peak_position',
    #                     data=measurement_data['ZeroPeakPosition'].values.reshape(
    #                         (len_x, len_y),
    #                         order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='channel index',
    #                     data_long_name='zero peak position',
    #                     additional_dims=None),
    #     make_data_array(data_name='zero_peak_frequency',
    #                     data=measurement_data['ZeroPeakFrequency'].values.reshape(
    #                         (len_x, len_y),
    #                         order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='counts',
    #                     data_long_name='zero peak frequency',
    #                     additional_dims=None),
    #     make_data_array(data_name='pulse_density',
    #                     data=measurement_data['PulseDensity'].values.reshape(
    #                         (len_x, len_y),
    #                         order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='',
    #                     data_long_name='pulse density',
    #                     additional_dims=None),
    #     make_data_array(data_name='amplification',
    #                     data=measurement_data['Amplification'].values.reshape(
    #                         (len_x, len_y),
    #                         order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='',
    #                     data_long_name='amplification',
    #                     additional_dims=None),
    #     make_data_array(data_name='shaping_time',
    #                     data=measurement_data['ShapingTime'].values.reshape(
    #                         (len_x, len_y),
    #                         order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='ms',
    #                     data_long_name='shaping time',
    #                     additional_dims=None),
    #     make_data_array(data_name='tube_voltage',
    #                     data=measurement_data['Voltage'].values.reshape((len_x, len_y),
    #                                                                     order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='kV',
    #                     data_long_name='tube voltage',
    #                     additional_dims=None),
    #     make_data_array(data_name='tube_current',
    #                     data=measurement_data['Current'].values.reshape((len_x, len_y),
    #                                                                     order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='$\\mu$ A',
    #                     data_long_name='tube current',
    #                     additional_dims=None),
    #     make_data_array(data_name='chamber_pressure',
    #                     data=measurement_data['ChamberPressure'].values.reshape(
    #                         (len_x, len_y),
    #                         order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='hPa',
    #                     data_long_name='chamber pressure',
    #                     additional_dims=None),
    #     make_data_array(data_name='measurement_datetime',
    #                     data=measurement_data['DateTime'].values.reshape((len_x, len_y),
    #                                                                      order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='',
    #                     data_long_name='date and time of measurement',
    #                     additional_dims=None),
    #     make_data_array(data_name='channel_energy_calibration_linear',
    #                     data=measurement_data['CalibAbs'].values.reshape((len_x, len_y),
    #                                                                      order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='keV / channel',
    #                     data_long_name='slope of energy-channel-calibration',
    #                     additional_dims=None),
    #     make_data_array(data_name='channel_energy_calibration_absolute',
    #                     data=measurement_data['CalibLin'].values.reshape((len_x, len_y),
    #                                                                      order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='keV',
    #                     data_long_name='offset of energy-channel-calibration',
    #                     additional_dims=None),
    #     make_data_array(data_name='channel_energy_calibration_linear_sigma',
    #                     data=measurement_data['SigmaAbs'].values.reshape((len_x, len_y),
    #                                                                      order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='keV / channel',
    #                     data_long_name='sigma of slope of energy-channel-calibration',
    #                     additional_dims=None),
    #     make_data_array(data_name='channel_energy_calibration_absolute_sigma',
    #                     data=measurement_data['SigmaLin'].values.reshape((len_x, len_y),
    #                                                                      order=order_letter),
    #                     x_dim=x_dim_array,
    #                     y_dim=y_dim_array,
    #                     data_units='keV',
    #                     data_long_name='sigma of offset of energy-channel-calibration',
    #                     additional_dims=None)]

    # # generate additional attribute dict (These should not change within the library --> take first)
    # additional_attrs_dict = {'detector_count': measurement_data['DetectorCount'].values[0],
    #                          'selected_detectors': measurement_data['SelectedDetectors'].values[0],
    #                          'tube_type': measurement_data['TubeType'].values[0],
    #                          'tube_number': measurement_data['TubeNumber'].values[0],
    #                          'tube_production_date': measurement_data['TubeProdDate'].values[0],
    #                          'tube_anode': measurement_data['Anode'].values[0],
    #                          'tube_incident_angle': measurement_data['TubeIncidentAngle'].values[0],
    #                          'tube_take_off_angle': measurement_data['TubeTakeOffAngle'].values[0],
    #                          'tube_window_atomic_number': measurement_data['TubeWindow,AtomicNumber'].values[0],
    #                          'tube_window_thickness': measurement_data['TubeWindow,Thickness'].values[0],
    #                          'optic': measurement_data['Optic'].values[0],
    #                          'spot_size': measurement_data['SpotSize'].values[0],
    #                          'exitation_angle': measurement_data['ExcitationAngle'].values[0],
    #                          'detection_angle': measurement_data['DetectionAngle'].values[0],
    #                          'excitation_path_length': measurement_data['ExcitationPathLength'].values[0],
    #                          'detection_path_length': measurement_data['DetectionPathLength'].values[0],
    #                          'solid_angle_detection': measurement_data['SolidAngleDetection'].values[0],
    #                          'azimut_angle_abs': measurement_data['AzimutAngleAbs'].values[0],
    #                          'det_azimut_angle': measurement_data['DetAzimutAngle'].values[0],
    #                          'tilt_angle': measurement_data['TiltAngle'].values[0],
    #                          'detector_spot_size': measurement_data['DetSpotSize'].values[0],
    #                          'atmosphere': measurement_data['Atmosphere'].values[0],
    #                          'channel_count': measurement_data['ChannelCount'].values[0],
    #                          'mean_lifetime': np.mean(measurement_data['LifeTime'].values),
    #                          'mean_realtime': np.mean(measurement_data['RealTime'].values),
    #                          'mean_tube_voltage': np.mean(measurement_data['Voltage'].values),
    #                          'mean_tube_current': np.mean(measurement_data['Current'].values),
    #                          'mean_pressure': np.mean(measurement_data['ChamberPressure'].values)}

    # measurement_ds = make_dataset(data_arrays=measurement_data_arrays_list,
    #                               data_type='',
    #                               library_id=library_infos[0]["identifiers"]["library_id"],
    #                               data_name='',
    #                               data_datetime=datetime.strptime(measurement_data['DateTime'].values[0],
    #                                                               '%Y-%m-%dT%H:%M:%S.%f'),
    #                               operator=required_params["operator"].split(' - ')[-1],
    #                               importing_user=user,
    #                               comment=required_params["comment"],
    #                               rating=None,
    #                               additional_attrs=additional_attrs_dict,
    #                               original_data=None,
    #                               )

    # return_ds_list = [measurement_ds]

    # # for now the results are only saved, if there is results for every single spectrum. Else the assignment of
    # # the results is not possible (easily)
    # library_layer_prop_list = []
    # if len(fit_bkg) == len(spectra):
    #     # Reformat pd.DataFrames into xr.DataSets to make the data cubes easily accessible
    #     layer_df = pd.DataFrame(layer_rows).replace(np.nan, 0).astype({'idx': int,
    #                                                                    'LayerName': str,
    #                                                                    'Element': str,
    #                                                                    'Atom': int,
    #                                                                    'XLine': str,
    #                                                                    'AtomPercent': float,
    #                                                                    'MassPercent': float,
    #                                                                    'NetIntens': int}).set_index(['idx',
    #                                                                                                  'LayerName',
    #                                                                                                  'Element'])
    #     layer_ds = layer_df.to_xarray()

    #     layer_prop_df = pd.DataFrame(layer_prop_rows).astype({'idx': int,
    #                                                           'LayerName': str,
    #                                                           'Density': float,
    #                                                           'Thickness': float,
    #                                                           'ThicknessError': float}).set_index(['idx',
    #                                                                                                'LayerName'])
    #     layer_prop_ds = layer_prop_df.to_xarray()

    #     # 'Element' entry was called 'Name' in the XML. Easiest way to change is here, I think
    #     roi_df = pd.DataFrame(roi_rows).rename(
    #         columns={'Name': 'Element'}).astype({'idx': int,
    #                                              'Element': str,
    #                                              'Atom': int,
    #                                              'Description': str,
    #                                              'Line': str,
    #                                              'Energy': float,
    #                                              'Width': float,
    #                                              'Counts': int,
    #                                              'NetCounts': int}).set_index(['idx', 'Element'])
    #     roi_ds = roi_df.to_xarray()

    #     deconvolution_df = pd.DataFrame(deconvolution_rows).astype({'idx': int,
    #                                                                 'Element': str,
    #                                                                 'Line': str,
    #                                                                 'Energy': float,
    #                                                                 'Counts': int}).set_index(['idx',
    #                                                                                            'Element',
    #                                                                                            'Line'])
    #     deconvolution_ds = deconvolution_df.to_xarray()

    #     # make dimension with all elements that were used for the fitting of the spectra
    #     element_dim_array = make_dimension_array(dim_name="element",
    #                                              dim_values=roi_ds.Element.data,
    #                                              dim_units="",
    #                                              dim_long_name="chemical element")
    #     no_of_elems = len(element_dim_array.data)

    #     # make dimension with all lines ('Union' of all lines in all elements)
    #     line_dim_array = make_dimension_array(dim_name="fluorescence_lines",
    #                                           dim_values=deconvolution_ds.Line.data,
    #                                           dim_units="",
    #                                           dim_long_name="X-ray fluorescence line")
    #     no_of_lines = len(line_dim_array.data)

    #     # The fit is (presumably) performed by evaluating the roi for specific elements,
    #     # then deconvoluting fluoresence lines
    #     # and then calculating layer results from this.
    #     # Thus it will ordered and referred like this here:

    #     roi_data_array_list = [
    #         make_data_array(data_name='atomic_number',
    #                         data=roi_ds['Atom'].data.reshape((len_x, len_y, no_of_elems),
    #                                                          order=order_letter),
    #                         x_dim=x_dim_array,
    #                         y_dim=y_dim_array,
    #                         data_units='',
    #                         data_long_name='atomic number',
    #                         additional_dims=[element_dim_array]),
    #         make_data_array(data_name='roi_description',
    #                         data=roi_ds['Description'].data.reshape((len_x, len_y, no_of_elems),
    #                                                                 order=order_letter),
    #                         x_dim=x_dim_array,
    #                         y_dim=y_dim_array,
    #                         data_units='',
    #                         data_long_name='description of the roi (Element and Line)',
    #                         additional_dims=[element_dim_array]),
    #         make_data_array(data_name='fluorescence_line',
    #                         data=roi_ds['Line'].data.reshape((len_x, len_y, no_of_elems),
    #                                                          order=order_letter),
    #                         x_dim=x_dim_array,
    #                         y_dim=y_dim_array,
    #                         data_units='',
    #                         data_long_name='fluorescence line',
    #                         additional_dims=[element_dim_array]),
    #         make_data_array(data_name='fluorescence_energy',
    #                         data=roi_ds['Energy'].data.reshape((len_x, len_y, no_of_elems),
    #                                                            order=order_letter),
    #                         x_dim=x_dim_array,
    #                         y_dim=y_dim_array,
    #                         data_units='keV',
    #                         data_long_name='fluorescence energy',
    #                         additional_dims=[element_dim_array]),
    #         make_data_array(data_name='roi_width',
    #                         data=roi_ds['Width'].data.reshape((len_x, len_y, no_of_elems),
    #                                                           order=order_letter),
    #                         x_dim=x_dim_array,
    #                         y_dim=y_dim_array,
    #                         data_units='keV',
    #                         data_long_name='width of roi',
    #                         additional_dims=[element_dim_array]),
    #         make_data_array(data_name='roi_counts',
    #                         data=roi_ds['Counts'].data.reshape((len_x, len_y, no_of_elems),
    #                                                            order=order_letter),
    #                         x_dim=x_dim_array,
    #                         y_dim=y_dim_array,
    #                         data_units='',
    #                         data_long_name='counts in roi',
    #                         additional_dims=[element_dim_array]),
    #         make_data_array(data_name='roi_net_counts',
    #                         data=roi_ds['NetCounts'].data.reshape((len_x, len_y, no_of_elems),
    #                                                               order=order_letter),
    #                         x_dim=x_dim_array,
    #                         y_dim=y_dim_array,
    #                         data_units='',
    #                         data_long_name='net counts in roi',
    #                         additional_dims=[element_dim_array])
    #     ]

    #     roi_ds = make_dataset(data_arrays=roi_data_array_list,
    #                           data_type='derived_data',
    #                           library_id=library_infos[0]["identifiers"]["library_id"],
    #                           data_name='_roi_results',
    #                           data_datetime=datetime.strptime(measurement_data['DateTime'].values[0],
    #                                                           '%Y-%m-%dT%H:%M:%S.%f'),
    #                           operator=required_params["operator"].split(' - ')[-1],
    #                           importing_user=user,
    #                           comment=required_params["comment"],
    #                           rating=None,
    #                           additional_attrs=None,
    #                           original_data=[library_infos[0]["identifiers"]["library_id"]
    #                                          + "/measurements/"
    #                                          + generate_knc_filename(measurement_ds)]
    #                           )

    #     # Make a dataset for the results of the deconvolution
    #     deconvolution_data_array_list = [
    #         make_data_array(data_name='fluorescence_line_energy',
    #                         data=deconvolution_ds['Energy'].data.reshape(
    #                             (len_x, len_y, no_of_elems,  no_of_lines), order=order_letter),
    #                         x_dim=x_dim_array,
    #                         y_dim=y_dim_array,
    #                         data_units='keV',
    #                         data_long_name='fluorescence line energy',
    #                         additional_dims=[element_dim_array, line_dim_array]),
    #         make_data_array(data_name='fluorescence_line_counts',
    #                         data=deconvolution_ds['Counts'].data.reshape(
    #                             (len_x, len_y, no_of_elems,  no_of_lines), order=order_letter),
    #                         x_dim=x_dim_array,
    #                         y_dim=y_dim_array,
    #                         data_units='keV',
    #                         data_long_name='fluorescence line counts',
    #                         additional_dims=[element_dim_array, line_dim_array]),
    #         make_data_array(data_name='background',
    #                         data=np.array(fit_bkg).reshape((len_x, len_y, len(energy_axis)),
    #                                                        order=order_letter),
    #                         x_dim=x_dim_array,
    #                         y_dim=y_dim_array,
    #                         data_units='counts',
    #                         data_long_name='fitted background',
    #                         additional_dims=[energy_dim_array]),
    #         make_data_array(data_name='deconvolution_method',
    #                         data=np.array(deconvolution_methods).reshape((len_x, len_y),
    #                                                                      order=order_letter),
    #                         x_dim=x_dim_array,
    #                         y_dim=y_dim_array,
    #                         data_units='',
    #                         data_long_name='method used for the deconvolution',
    #                         additional_dims=None)
    #     ]

    #     deconvolution_ds = make_dataset(data_arrays=deconvolution_data_array_list,
    #                                     data_type='derived_data',
    #                                     library_id=library_infos[0]["identifiers"]["library_id"],
    #                                     data_name='_deconvolution_results',
    #                                     data_datetime=datetime.strptime(measurement_data['DateTime'].values[0],
    #                                                                     '%Y-%m-%dT%H:%M:%S.%f'),
    #                                     operator=required_params["operator"].split(' - ')[-1],
    #                                     importing_user=user,
    #                                     comment=required_params["comment"],
    #                                     rating=None,
    #                                     additional_attrs=None,
    #                                     original_data=[library_infos[0]["identifiers"]["library_id"]
    #                                                    + "/derived_data/"
    #                                                    + generate_knc_filename(roi_ds)]
    #                                     )

    #     # Make a dataset for the layer_results
    #     layer_ds_list = []  # List of Datasets (one entry per layer)
    #     for layer_name in layer_ds.LayerName.data:
    #         layer_data_array_list = [
    #             make_data_array(data_name='atomic_number',
    #                             data=layer_ds.sel(LayerName=layer_name)['Atom'].values.reshape(
    #                                 (len_x, len_y, no_of_elems), order=order_letter),
    #                             x_dim=x_dim_array,
    #                             y_dim=y_dim_array,
    #                             data_units='',
    #                             data_long_name='atomic number',
    #                             additional_dims=[element_dim_array]),
    #             make_data_array(data_name='fluorescence_line',
    #                             data=layer_ds.sel(LayerName=layer_name)['XLine'].values.reshape(
    #                                 (len_x, len_y, no_of_elems), order=order_letter),
    #                             x_dim=x_dim_array,
    #                             y_dim=y_dim_array,
    #                             data_units='',
    #                             data_long_name='fluorescence line',
    #                             additional_dims=[element_dim_array]),
    #             make_data_array(data_name='atom_percent',
    #                             data=layer_ds.sel(LayerName=layer_name)['AtomPercent'].values.reshape(
    #                                 (len_x, len_y, no_of_elems), order=order_letter),
    #                             x_dim=x_dim_array,
    #                             y_dim=y_dim_array,
    #                             data_units='%',
    #                             data_long_name='atomic percent',
    #                             additional_dims=[element_dim_array]),
    #             make_data_array(data_name='weight_percent',
    #                             data=layer_ds.sel(LayerName=layer_name)['MassPercent'].values.reshape(
    #                                 (len_x, len_y, no_of_elems), order=order_letter),
    #                             x_dim=x_dim_array,
    #                             y_dim=y_dim_array,
    #                             data_units='%',
    #                             data_long_name='weight percent',
    #                             additional_dims=[element_dim_array]),
    #             make_data_array(data_name='net_intensity',
    #                             data=layer_ds.sel(LayerName=layer_name)['NetIntens'].values.reshape(
    #                                 (len_x, len_y, no_of_elems), order=order_letter),
    #                             x_dim=x_dim_array,
    #                             y_dim=y_dim_array,
    #                             data_units='counts',
    #                             data_long_name='net intensity',
    #                             additional_dims=[element_dim_array]),
    #             make_data_array(data_name='layer_density',
    #                             data=layer_prop_ds.sel(LayerName=layer_name)['Density'].values.reshape(
    #                                 (len_x, len_y), order=order_letter),
    #                             x_dim=x_dim_array,
    #                             y_dim=y_dim_array,
    #                             data_units='g/cm$^3$',
    #                             data_long_name='layer density',
    #                             additional_dims=None),
    #             make_data_array(data_name='layer_thickness',
    #                             data=layer_prop_ds.sel(LayerName=layer_name)['Thickness'].values.reshape(
    #                                 (len_x, len_y), order=order_letter),
    #                             x_dim=x_dim_array,
    #                             y_dim=y_dim_array,
    #                             data_units='$\\mu$m',
    #                             data_long_name='layer thickness',
    #                             additional_dims=None),
    #             make_data_array(data_name='layer_thickness_error',
    #                             data=layer_prop_ds.sel(LayerName=layer_name)['ThicknessError'].values.reshape(
    #                                 (len_x, len_y), order=order_letter),
    #                             x_dim=x_dim_array,
    #                             y_dim=y_dim_array,
    #                             data_units='$\\mu$m',
    #                             data_long_name='layer thickness error',
    #                             additional_dims=None)]

    #         layer_ds_list.append(
    #             make_dataset(data_arrays=layer_data_array_list,
    #                          data_type='derived_data',
    #                          library_id=library_infos[0]["identifiers"]["library_id"],
    #                          data_name='_' + layer_name + '_layer_results',
    #                          data_datetime=datetime.strptime(measurement_data['DateTime'].values[0],
    #                                                          '%Y-%m-%dT%H:%M:%S.%f'),
    #                          operator=required_params["operator"].split(' - ')[-1],
    #                          importing_user=user,
    #                          comment=required_params["comment"],
    #                          rating=None,
    #                          additional_attrs=None,
    #                          original_data=[library_infos[0]["identifiers"]["library_id"]
    #                                         + "/derived_data/"
    #                                         + generate_knc_filename(deconvolution_ds)]
    #                          )
    #         )

    #     # show dialog for assignment of XRF layers to library_layers
    #     json_layer_list = [layer['name'] + ' - ' + layer['type'] for layer in library_infos[0]['layers']]
    #     extracted_results_list = []
    #     for idx, layer_name in enumerate(layer_ds.LayerName.data):
    #         extracted_results_list.append(layer_name)
    #         library_layer_prop_list.append([])
    #         for element in layer_ds.Element.data:
    #             subset = layer_ds.sel({'LayerName': layer_name, 'Element': element})
    #             layer_prop_min = np.round(subset['AtomPercent'].data.min()*100, 2)
    #             layer_prop_max = np.round(subset['AtomPercent'].data.max() * 100, 2)
    #             if not np.isnan(layer_prop_min) and not np.isnan(layer_prop_max):
    #                 extracted_results_list[-1] += ':\r\n' \
    #                                               + element + ' at%: ['\
    #                                               + "{:.2f}".format(layer_prop_min)\
    #                                               + ', '\
    #                                               + "{:.2f}".format(layer_prop_max)\
    #                                               + ']'
    #                 library_layer_prop_list[-1].append(
    #                     LibraryLayerProperty(library_id=library_infos[0]["identifiers"]["library_id"],
    #                                          layer_id=-1,
    #                                          keys=['compositional', 'atomic_percent', element],
    #                                          value=(layer_prop_min, layer_prop_max),
    #                                          unit='%',
    #                                          source=(library_infos[0]["identifiers"]["library_id"] +
    #                                                  "/derived_data/" + generate_knc_filename(layer_ds_list[idx]))
    #                                          ))

    #     # dlg = LayerAssignmentDialog(combo_content=json_layer_list, xrf_results=extracted_results_list)
    #     # if dlg.exec():
    #     #     # make a copy to refer to after items have been deleted from the original list
    #     #     library_layer_prop_list_copy = library_layer_prop_list.copy()
    #     #     for idx, cb_layer_select in enumerate(dlg.cb_list):
    #     #         selected_json_layer_id = cb_layer_select.currentIndex() - 1
    #     #         if cb_layer_select.currentText() != dlg.none_text:
    #     #             for layer_element in library_layer_prop_list[idx]:
    #     #                 layer_element.layer_id = selected_json_layer_id
    #     #         else:
    #     #             library_layer_prop_list.remove(library_layer_prop_list_copy[idx])
    #     #     library_layer_prop_list = sum(library_layer_prop_list, [])  # flattens the list

    #     if roi_ds is not None:
    #         return_ds_list.append(roi_ds)
    #     if deconvolution_ds is not None:
    #         return_ds_list.append(deconvolution_ds)
    #     if layer_ds_list != []:
    #         return_ds_list.extend(layer_ds_list)

    # return return_ds_list, library_layer_prop_list
